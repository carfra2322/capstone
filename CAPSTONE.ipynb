{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required library installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install langdetect -y\n",
    "#!conda install boto3 -y\n",
    "#!conda activate MLenv\n",
    "#!conda install --name MLenv pip -y\n",
    "#!pip install ray\n",
    "#!pip uninstall ray -y\n",
    "#!conda install dask -y\n",
    "#! conda install numpy --name MLenv -y\n",
    "#! pip3 install numpy --upgrade\n",
    "#!conda install modin -y\n",
    "#!conda uninstall --name MLenv modin -y\n",
    "#!pip install modin\n",
    "#!pip uninstall modin -y\n",
    "#!pip install scispacy\n",
    "#!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\n",
    "#!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz\n",
    "#!pip install dask-ml\n",
    "# see https://allenai.github.io/scispacy/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://towardsdatascience.com/getting-started-with-pyspark-on-amazon-emr-c85154b6b921\n",
    "https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf    \n",
    "    \n",
    "https://aws.amazon.com/blogs/machine-learning/deploying-machine-learning-models-as-serverless-apis/\n",
    "    \n",
    "    \n",
    "https://www.youtube.com/watch?v=SEQbb8w7VTw\n",
    "    \n",
    "buy stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modin.pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "for p in sys.path:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "from time import sleep\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "#import dask.dataframe as dd\n",
    "import glob\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) DATA EXPLORATION\n",
    "\n",
    "### Select a subset of JSON files in order to start testing your code \n",
    "### Approach:\n",
    "\n",
    "* 10 json files\n",
    "* 50 json files\n",
    "* 1000 json files\n",
    "* 5000 json files\n",
    "* 10000 json files\n",
    "* 50000 json files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "118839 JSON documents \n",
      "\n",
      "CPU times: user 241 ms, sys: 262 ms, total: 502 ms\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# LOADS PATH OF JSON OBJECTS\n",
    "root_path = 'archive/document_parses/pdf_json'\n",
    "all_json = glob.glob(f'{root_path}/**/*.json', recursive=True)\n",
    "\n",
    "print('''\n",
    "{} JSON documents \n",
    "'''.format(len(all_json)))\n",
    "\n",
    "subset_json = all_json[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ❌Option 1: Parse JSON to PD DataFrame \n",
    "\n",
    "This was my first approach, but parsing 50,000 records took approximately 30 minutes.\n",
    "I went ahead and explored using distributed, parallel processing by using Ray Libary (Option 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create columns and DataFrame\n",
    "# Approach is to create a dataframe with the size of the number of json files\n",
    "# one row per json file \n",
    "\n",
    "def json2DF(json_lst):\n",
    "    \n",
    "    \n",
    "   \n",
    "    length = len(json_lst)\n",
    "    cols = ['paper_id', 'title','body_text','abstract','authors', 'url', 'doi']\n",
    "    int_covidDF = pd.DataFrame(columns=cols, index=range(length))\n",
    "    #int_covidDF = dd.from_pandas(tempDF, npartitions=3)\n",
    "\n",
    "    # Iterate through all json objects \n",
    "    for index, path in enumerate(tqdm(json_lst, desc='JSON Files being processed')):\n",
    "        #print(index)\n",
    "        #load json object\n",
    "        try:\n",
    "            with open(path) as f:\n",
    "                d = json.load(f)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # Get the paper ID \n",
    "        int_covidDF.loc[index]['paper_id'] = d['paper_id']\n",
    "\n",
    "        # Some json files do not have title so the data has to be pulled from metadata dataframe    \n",
    "        meta_data = meta_dataDF.loc[meta_dataDF['sha'] == d['paper_id']]\n",
    "\n",
    "        try:\n",
    "            #print(d['metadata']['title'][:5])\n",
    "            if len(d['metadata']['title']) > 0:\n",
    "                int_covidDF.loc[index]['title'] = d['metadata']['title']\n",
    "                #print('got it from json')\n",
    "            elif len(meta_data['title'])>0:\n",
    "                int_covidDF.loc[index]['title'] = meta_data['title']\n",
    "                #print('got it from meta')\n",
    "        except:\n",
    "            int_covidDF.loc[index]['title'] = None\n",
    "\n",
    "\n",
    "        #body of text \n",
    "        int_covidDF.loc[index]['body_text'] = ''.join(pd.json_normalize(d, record_path=['body_text']).text)\n",
    "\n",
    "\n",
    "        # abstract\n",
    "        try:\n",
    "            if len(pd.json_normalize(d, record_path=['abstract'])) > 0:          \n",
    "                int_covidDF.loc[index]['abstract'] = ''.join(pd.json_normalize(d, record_path=['abstract'])['text'])\n",
    "            elif len(meta_data['abstract'].values[0]) > 0:\n",
    "                int_covidDF.loc[index]['abstract'] = ''.join(meta_data['abstract'].values)\n",
    "                #print(\"here\", index)\n",
    "        except:        \n",
    "            int_covidDF.loc[index]['abstract'] = 'No abstract was found'\n",
    "\n",
    "\n",
    "        # Authors first and last name\n",
    "        try:\n",
    "            if len(pd.json_normalize(d, record_path=['metadata','authors'])['first'].values + ' ' + pd.json_normalize(d, record_path=['metadata','authors'])['last'].values) > 0:\n",
    "                int_covidDF.loc[index]['authors'] = pd.json_normalize(d, record_path=['metadata','authors'])['first'].values + ' ' + pd.json_normalize(d, record_path=['metadata','authors'])['last'].values\n",
    "\n",
    "            elif len(meta_data['authors'] > 0):\n",
    "                int_covidDF.loc[index]['authors'] = meta_data['authors']\n",
    "\n",
    "        except:\n",
    "\n",
    "            int_covidDF.loc[index]['authors'] = ''\n",
    "\n",
    "\n",
    "        if len(meta_data['url'].values) > 0:\n",
    "            int_covidDF.loc[index]['url'] = meta_data['url'].values\n",
    "        else:\n",
    "            int_covidDF.loc[index]['url'] = np.nan\n",
    "\n",
    "        if len(meta_data['doi'].values) > 0:\n",
    "            int_covidDF.loc[index]['doi'] = meta_data['doi'].values\n",
    "        else:\n",
    "            int_covidDF.loc[index]['doi'] = np.nan\n",
    "            \n",
    "    return int_covidDF\n",
    "\n",
    "        # Used to display the progress\n",
    "        # There are around 118,839 json documents divide by the times you want to print\n",
    "        #if index % (len(all_json) // 5 ) == 0:\n",
    "        #    print('{} out of {} processed'.format(index, len(all_json)))\n",
    "    \n",
    "\n",
    "covidDF = json2DF(subset_json)    \n",
    "    \n",
    "covidDF.info()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Option 2: Parse JSON to a List of Dictionaries\n",
    "\n",
    "In this approached I parsed the JSON file into a list of dictionaries and distributed the load into the number of cores my computer could handle.  This was done using Ray library, which can be used with a local computer but can also be used with a distributed processing system such as Hadoop EMR. \n",
    "\n",
    "This reduced the processing time from ~25 minutes to ~3 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON2DICT function defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 278 ms, sys: 103 ms, total: 381 ms\n",
      "Wall time: 969 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import ray\n",
    "\n",
    "\n",
    "# Define the functions\n",
    "# Decorate the function so that we can trigger it remotely\n",
    "@ray.remote\n",
    "def json2Dict(json_lst, df):\n",
    "    \n",
    "    #print('THIS IS THE VERSION ', pd.__version__, os.path)\n",
    "    \n",
    "    meta_dataDF = df\n",
    "    #meta_dataDF = pd.read_csv('archive/metadata.csv')\n",
    "    tempDict = {'paper_id':[], 'title':[],'body_text':[],'abstract':[],'authors':[], 'url':[], 'doi':[]}\n",
    "   \n",
    "    length = len(json_lst)\n",
    "    \n",
    "    #cols = ['paper_id', 'title','body_text','abstract','authors', 'url', 'doi']\n",
    "    #int_covidDF = pd.DataFrame(columns=cols, index=range(length))\n",
    "    \n",
    "\n",
    "    # Iterate through all json objects \n",
    "    for index, path in tqdm(enumerate(json_lst)):\n",
    "        #print(index)\n",
    "        #load json object\n",
    "        try:\n",
    "            with open(path) as f:\n",
    "                data = json.load(f)\n",
    "        except:\n",
    "            print(index, 'file not processed')\n",
    "            continue\n",
    "\n",
    "        # Get the paper ID \n",
    "        tempDict['paper_id'].append(data['paper_id'])\n",
    "\n",
    "        # Some json files do not have title so the data has to be pulled from metadata dataframe    \n",
    "        meta_data = meta_dataDF.loc[meta_dataDF['sha'] == data['paper_id']]\n",
    "\n",
    "        try:\n",
    "            #print(d['metadata']['title'][:5])\n",
    "            if len(data['metadata']['title']) > 0:\n",
    "                tempDict['title'].append(data['metadata']['title'])\n",
    "                #print('got it from json')\n",
    "            elif len(meta_data['title'])>0:\n",
    "                tempDict['title'].append(meta_data['title'])\n",
    "                #print('got it from meta')\n",
    "        except:\n",
    "            tempDict['title'].append(None)\n",
    "\n",
    "\n",
    "        # Body of text \n",
    "        tempDict['body_text'].append(''.join(pd.json_normalize(data, record_path=['body_text']).text))\n",
    "\n",
    "\n",
    "        # abstract\n",
    "        try:\n",
    "            if len(pd.json_normalize(data, record_path=['abstract'])) > 0:          \n",
    "                tempDict['abstract'].append(''.join(pd.json_normalize(data, record_path=['abstract'])['text']))\n",
    "            elif len(meta_data['abstract'].values[0]) > 0:\n",
    "                tempDict['abstract'].append(''.join(meta_data['abstract'].values))\n",
    "                #print(\"here\", index)\n",
    "        except:        \n",
    "            tempDict['abstract'].append('No abstract was found')\n",
    "\n",
    "\n",
    "        # Authors first and last name\n",
    "        try:\n",
    "            if len(pd.json_normalize(data, record_path=['metadata','authors'])['first'].values + ' ' + pd.json_normalize(data, record_path=['metadata','authors'])['last'].values) > 0:\n",
    "                tempDict['authors'].append(pd.json_normalize(data, record_path=['metadata','authors'])['first'].values + ' ' + pd.json_normalize(data, record_path=['metadata','authors'])['last'].values)\n",
    "\n",
    "            elif len(meta_data['authors'] > 0):\n",
    "                tempDict['authors'].append(meta_data['authors'])\n",
    "\n",
    "        except:\n",
    "\n",
    "            tempDict['authors'].append('')\n",
    "\n",
    "\n",
    "        if len(meta_data['url'].values) > 0:\n",
    "            tempDict['url'].append(meta_data['url'].values)\n",
    "        else:\n",
    "            tempDict['url'].append(np.nan)\n",
    "\n",
    "        if len(meta_data['doi'].values) > 0:\n",
    "            tempDict['doi'].append(meta_data['doi'].values)\n",
    "        else:\n",
    "            tempDict['doi'].append(np.nan)\n",
    "            \n",
    "    return tempDict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray: Parallel and distributed computing\n",
    "### JSON2DICT function executed in a parallel distributed manner\n",
    "\n",
    "Approach: \n",
    "\n",
    "* Get the number of records to be processed\n",
    "* Get the number of cores in the machine (could be number of nodes in a cluster)\n",
    "* Divide number of records by the cores, to get how many records will be processed per core\n",
    "* Loop through all the records to be processed by increments of the number of records processed per core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-16 00:55:45,155\tINFO services.py:1166 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "<string>:2: DtypeWarning: Columns (4,5,6,13,14,16) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4592dc067334c6995def6644d02a8ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Sending parallel processes'), FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea84962948c14cf5a848ff98fe662f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Processing distributed load'), FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=8525)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8521)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8519)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8514)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8515)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8522)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8517)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8516)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8520)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8523)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8518)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8524)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8525)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8523)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8517)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8521)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8514)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8519)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8516)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8520)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8524)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8522)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8515)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8518)\u001b[0m \n",
      "\n",
      "CPU times: user 7.89 s, sys: 2.14 s, total: 10 s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "# Initialize the ray parallel processing\n",
    "ray.init()\n",
    "\n",
    "# This puts the dataframe in the distributed ray process \n",
    "dist_meta_dataDF = ray.put(pd.read_csv('archive/metadata.csv'))\n",
    "\n",
    "\n",
    "# Number of cores in machine (12)\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "\n",
    "# Number of records to be processed\n",
    "length = len(subset_json)\n",
    "\n",
    "# Records that will be processed by core\n",
    "# divide by the length \n",
    "sizeByCore = int(length/cores)\n",
    "\n",
    "# Initialize the list that will capture the processed JSON results\n",
    "lst = []\n",
    "\n",
    "# Use this to set the index of the JSON records to be processed \n",
    "# The following for function will loop through all the records \n",
    "# it will be incremented by the number of records by core \n",
    "# Since it is a parallel process, it will use all cores of the machines to process the records distributed \n",
    "# \"evenly\"\n",
    "start = 0\n",
    "for i in tqdm(range(sizeByCore, length, sizeByCore), desc=\"Sending parallel processes\"):    \n",
    "    if length-(start+sizeByCore) > sizeByCore:\n",
    "#         print('''\n",
    "#         if {}-({}+{}) > {}\n",
    "#         '''.format(length,start,sizeByCore,sizeByCore))\n",
    "#         print('Processing {}-{} records'.format(start+1, start+sizeByCore))\n",
    "        x_id = json2Dict.remote(subset_json[start:start+sizeByCore], dist_meta_dataDF)\n",
    "    else:\n",
    "#         print('Processing Last {}-{} records'.format(start, length))\n",
    "        x_id = json2Dict.remote(subset_json[start:length], dist_meta_dataDF)        \n",
    "    lst.append(x_id)\n",
    "    start+=sizeByCore\n",
    "    \n",
    "# Process the remaining files \n",
    "#lst.append(json2Dict.remote(subset_json[start:], dist_meta_dataDF))\n",
    "\n",
    "# Block until the tasks are done and get the results.\n",
    "# x = ray.get(lst)\n",
    "new_lst = []\n",
    "for i in tqdm(lst, desc=\"Processing distributed load\"):\n",
    "    new_lst.append(ray.get(i))\n",
    "\n",
    "# Shutdown ray     \n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD THE LIST OF DICTIONARIES INTO A DATAFRAME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "833\n",
      "833\n",
      "833\n",
      "833\n",
      "833\n",
      "833\n",
      "833\n",
      "833\n",
      "833\n",
      "833\n",
      "833\n",
      "837\n"
     ]
    }
   ],
   "source": [
    "for i in new_lst:\n",
    "    print(len(i['paper_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b825573b143245cc826322eef9e13144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='List of dictionaries into DataFrame'), FloatProgress(value=0.0, max=11.0), HTML(val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   paper_id   10000 non-null  object\n",
      " 1   title      9876 non-null   object\n",
      " 2   body_text  10000 non-null  object\n",
      " 3   abstract   10000 non-null  object\n",
      " 4   authors    10000 non-null  object\n",
      " 5   url        8885 non-null   object\n",
      " 6   doi        8885 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 547.0+ KB\n"
     ]
    }
   ],
   "source": [
    "covidDF = pd.DataFrame.from_dict(new_lst[0], orient='index').transpose()\n",
    "\n",
    "for i in tqdm(range(1, len(new_lst)), desc='List of dictionaries into DataFrame'):\n",
    "    covidDF = covidDF.append(pd.DataFrame.from_dict(new_lst[i], orient='index').transpose(), ignore_index=True)\n",
    "\n",
    "covidDF.info()\n",
    "covidDF.head()\n",
    "\n",
    "del new_lst #Remove list from memory once it's loaded in DF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) FEATURE ENGINEERING\n",
    "\n",
    "Will add the following features:\n",
    "\n",
    "* **abstract_cnt**    - count number of words in abstract feature\n",
    "* **body_cnt**        - count the number of words in body\n",
    "* **unique_body_cnt** - count the number of unique words in body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hectorcarrillo/anaconda3/lib/python3.7/site-packages/tqdm/std.py:703: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920d161ac61644a7baecb8b086446ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Finding Text'), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01c5488a6784b2a9659f264e3cd7bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Finding Text'), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e4f7d610c04dff9dcd59223f5a3a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Finding Text'), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 29.5 s, sys: 477 ms, total: 30 s\n",
      "Wall time: 29.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import re\n",
    "# Used regex to find words \n",
    "# Then count them \n",
    "tqdm.pandas(desc=\"Finding Text\")\n",
    "\n",
    "covidDF['abstract_cnt'] = covidDF['abstract'].progress_apply(lambda text: len(re.findall(r'\\w+', text)))\n",
    "covidDF['body_cnt'] = covidDF['body_text'].progress_apply(lambda text: len(re.findall(r'\\w+', text)))\n",
    "covidDF['unique_body_cnt'] = covidDF['body_text'].progress_apply(lambda text: len(set(re.findall(r'\\w+', text))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors and abstract have fields with blank spaces, so we cannot detect null values in empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   paper_id         10000 non-null  object\n",
      " 1   title            9876 non-null   object\n",
      " 2   body_text        10000 non-null  object\n",
      " 3   abstract         10000 non-null  object\n",
      " 4   authors          10000 non-null  object\n",
      " 5   url              8885 non-null   object\n",
      " 6   doi              8885 non-null   object\n",
      " 7   abstract_cnt     10000 non-null  int64 \n",
      " 8   body_cnt         10000 non-null  int64 \n",
      " 9   unique_body_cnt  10000 non-null  int64 \n",
      "dtypes: int64(3), object(7)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "covidDF.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert empty strings into Null values so that we can count them \n",
    "\n",
    "* run info to see null values again\n",
    "* we can see that the lowest count for non-null values is doi and url\n",
    "* these are essential reference fields so it is safe to remove all non-null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   paper_id         10000 non-null  object\n",
      " 1   title            9876 non-null   object\n",
      " 2   body_text        10000 non-null  object\n",
      " 3   abstract         10000 non-null  object\n",
      " 4   authors          9002 non-null   object\n",
      " 5   url              8885 non-null   object\n",
      " 6   doi              8885 non-null   object\n",
      " 7   abstract_cnt     10000 non-null  int64 \n",
      " 8   body_cnt         10000 non-null  int64 \n",
      " 9   unique_body_cnt  10000 non-null  int64 \n",
      "dtypes: int64(3), object(7)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "covidDF = covidDF.replace(r'^\\s*$', np.nan, regex=True)\n",
    "covidDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ABSTRACT =====\n",
      "\n",
      "count                     10000\n",
      "unique                     8254\n",
      "top       No abstract was found\n",
      "freq                       1698\n",
      "Name: abstract, dtype: object\n",
      "\n",
      "===== BODY TEXT =====\n",
      "\n",
      "count                                                 10000\n",
      "unique                                                 9981\n",
      "top       The nuclear receptor heterodimers of liver X r...\n",
      "freq                                                      7\n",
      "Name: body_text, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for unique values \n",
    "print('''\n",
    "===== ABSTRACT =====\n",
    "\n",
    "{}\n",
    "\n",
    "===== BODY TEXT =====\n",
    "\n",
    "{}\n",
    "'''.format(covidDF['abstract'].describe(include='all'), covidDF['body_text'].describe(include='all')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DROP DUPLICATES\n",
    "\n",
    "We will work with body text, and care about body_text being unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 706 ms, sys: 80.9 ms, total: 787 ms\n",
      "Wall time: 788 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count                                                  9981\n",
       "unique                                                 9981\n",
       "top       Coronavirus Disease-2019 (COVID- 19) is an acu...\n",
       "freq                                                      1\n",
       "Name: body_text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "covidDF.drop_duplicates(['body_text'], inplace=True)\n",
    "covidDF['body_text'].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract_cnt</th>\n",
       "      <th>body_cnt</th>\n",
       "      <th>unique_body_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9981.000000</td>\n",
       "      <td>9981.000000</td>\n",
       "      <td>9981.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>187.882978</td>\n",
       "      <td>3976.883078</td>\n",
       "      <td>998.939184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>196.449414</td>\n",
       "      <td>6909.190702</td>\n",
       "      <td>799.380424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>73.000000</td>\n",
       "      <td>1520.000000</td>\n",
       "      <td>577.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>174.000000</td>\n",
       "      <td>3006.000000</td>\n",
       "      <td>887.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>256.000000</td>\n",
       "      <td>4814.000000</td>\n",
       "      <td>1235.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5538.000000</td>\n",
       "      <td>243113.000000</td>\n",
       "      <td>14850.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       abstract_cnt       body_cnt  unique_body_cnt\n",
       "count   9981.000000    9981.000000      9981.000000\n",
       "mean     187.882978    3976.883078       998.939184\n",
       "std      196.449414    6909.190702       799.380424\n",
       "min        0.000000       1.000000         1.000000\n",
       "25%       73.000000    1520.000000       577.000000\n",
       "50%      174.000000    3006.000000       887.000000\n",
       "75%      256.000000    4814.000000      1235.000000\n",
       "max     5538.000000  243113.000000     14850.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covidDF.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8876 entries, 0 to 9998\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   paper_id         8876 non-null   object\n",
      " 1   title            8761 non-null   object\n",
      " 2   body_text        8876 non-null   object\n",
      " 3   abstract         8876 non-null   object\n",
      " 4   authors          8034 non-null   object\n",
      " 5   url              8876 non-null   object\n",
      " 6   doi              8876 non-null   object\n",
      " 7   abstract_cnt     8876 non-null   int64 \n",
      " 8   body_cnt         8876 non-null   int64 \n",
      " 9   unique_body_cnt  8876 non-null   int64 \n",
      "dtypes: int64(3), object(7)\n",
      "memory usage: 762.8+ KB\n"
     ]
    }
   ],
   "source": [
    "covidDF.dropna(subset=['url','doi'], inplace=True)\n",
    "covidDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETECT LANGUAGE \n",
    "\n",
    "In order to run the model, we will only process records that are in english.\n",
    "\n",
    "We will need to detect the language, and I will evaluate 2 options:\n",
    "\n",
    "* langdetect (https://pypi.org/project/langdetect/) \n",
    "* AWS comprehend (price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❌ AWS COMPREHEND API\n",
    "\n",
    "* pricing Language Detection\t$0.0001 per unit \n",
    "* NLP requests are measured in units of 100 characters, \n",
    "* with a 3 unit (300 character) minimum charge per request.\n",
    "\n",
    "**Free Tier**\n",
    "* 50K UNITS OF TEXT (5M CHARACTERS)\n",
    "* We have 118,839 JSON items\n",
    "* Leaves us at 68,839 x $0.0001 = $6.00 USD (max cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# see how to set credentials \n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\n",
    "# Uses my computer's locally stored credentials for AWS in order to make the calls to the api \n",
    "\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "session = boto3.Session(profile_name='amplify-ecommerce1')\n",
    "comprehend = session.client('comprehend')\n",
    "\n",
    "\n",
    "\n",
    "# Iterate through records\n",
    "\n",
    "for i in tqdm(covidDF.iloc[:10]['body_text']):\n",
    "    \n",
    "    response = comprehend.detect_dominant_language(Text = i[:300])\n",
    "    language = response['Languages'][0]['LanguageCode']\n",
    "    \n",
    "    \n",
    "    print('''\n",
    "    Language detected: {}\n",
    "    Full response: {}\n",
    "    Sample text: {}        \n",
    "    '''.format(language, response, i[:50] ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ LANGDETECT LIBRARY (Preferred)\n",
    "\n",
    "* Faster and freee\n",
    "\n",
    "\n",
    "**Normal Execution :** \n",
    "\n",
    "CPU times: user 2min 35s, sys: 1.98 s, total: 2min 37s\n",
    "\n",
    "Wall time: 2min 36s\n",
    "\n",
    "**vs**\n",
    "\n",
    "**Parallel/Distributed Execution :**\n",
    "\n",
    "CPU times: user 2.46 s, sys: 4.52 s, total: 6.97 s\n",
    "\n",
    "Wall time: 48.3 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.01 ms, sys: 3.39 ms, total: 10.4 ms\n",
      "Wall time: 13.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "# Define the functions\n",
    "# Decorate the function so that we can trigger it remotely\n",
    "@ray.remote(num_returns=2)\n",
    "def detectLanguage(textBody):\n",
    "    \n",
    "    # For consistent results\n",
    "    DetectorFactory.seed = 0\n",
    "    \n",
    "    # To store issues\n",
    "    issues = {'paper_id':[], 'body_text':[]}\n",
    "\n",
    "    # Initialize List to store languages\n",
    "    language = []\n",
    "\n",
    "    # Initialize an issues dictionary to store the paper_id and the text of the body \n",
    "    # This can be used to remove incorrectly formatted text corpus \n",
    "    \n",
    "\n",
    "    # Iterate through records\n",
    "    for index, text in enumerate(tqdm(textBody)):\n",
    "\n",
    "        try:\n",
    "            language.append(detect(text[:300]))\n",
    "        except:\n",
    "            try:\n",
    "                language.append(detect(text[-300:]))\n",
    "            except:\n",
    "                language.append('No text found')\n",
    "                issues['paper_id'].append(index)\n",
    "                issues['body_text'].append(text)\n",
    "                print('No text was found at index {}'.format(index))\n",
    "    \n",
    "    return language, issues\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray: Parallel and distributed computing\n",
    "### JSON2DICT function executed in a parallel distributed manner\n",
    "\n",
    "Approach: \n",
    "\n",
    "* Get the number of records to be processed\n",
    "* Get the number of cores in the machine (could be number of nodes in a cluster)\n",
    "* Divide number of records by the cores, to get how many records will be processed per core\n",
    "* Loop through all the records to be processed by increments of the number of records processed per core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8876"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(covidDF['body_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-16 00:59:10,638\tINFO services.py:1166 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7254aeb32ca74f67893ef2678716fab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Sending parallel processes'), FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0-739 records\n",
      "Processing 739-1478 records\n",
      "Processing 1478-2217 records\n",
      "Processing 2217-2956 records\n",
      "Processing 2956-3695 records\n",
      "Processing 3695-4434 records\n",
      "Processing 4434-5173 records\n",
      "Processing 5173-5912 records\n",
      "Processing 5912-6651 records\n",
      "Processing 6651-7390 records\n",
      "Processing 7390-8129 records\n",
      "Processing remaining 8129-8876 records\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f5b839fbca48d8a391b157abb03bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Unpacking distributed load for languages'), FloatProgress(value=0.0, max=12.0), HTM…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=8599)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=739.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8595)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=739.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8594)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=739.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8598)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=739.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8597)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=739.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8605)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=739.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8601)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=739.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8596)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=739.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8602)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=747.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8603)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=739.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8600)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=739.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8604)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=739.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8599)\u001b[0m No text was found at index 29\n",
      "\u001b[2m\u001b[36m(pid=8604)\u001b[0m No text was found at index 78\n",
      "\u001b[2m\u001b[36m(pid=8599)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8595)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8598)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8605)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8601)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8603)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8604)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8594)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8597)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8596)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8600)\u001b[0m \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11904063711492bad2dd968b3190746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Unpacking distributed load for issues'), FloatProgress(value=0.0, max=12.0), HTML(v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=8602)\u001b[0m \n",
      "\n",
      "CPU times: user 633 ms, sys: 557 ms, total: 1.19 s\n",
      "Wall time: 12.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# initialize ray\n",
    "ray.init()\n",
    "\n",
    "# Number of records to be processed\n",
    "length = len(covidDF['body_text'])\n",
    "\n",
    "# Records that will be processed by core\n",
    "# divide by the length \n",
    "sizeByCore = int(length/cores)\n",
    "\n",
    "# Initialize the list that will capture the processed results\n",
    "lst = []\n",
    "\n",
    "# Issues List \n",
    "issues = []\n",
    "\n",
    "# Use this to set the index of the JSON records to be processed \n",
    "# The following for function will loop through all the records \n",
    "# it will be incremented by the number of records by core \n",
    "# Since it is a parallel process, it will use all cores of the machines to process the records distributed \n",
    "# \"evenly\"\n",
    "start = 0\n",
    "for i in tqdm(range(sizeByCore, length, sizeByCore), desc=\"Sending parallel processes\"):\n",
    "    if length-(start+sizeByCore) > sizeByCore:\n",
    "        print('Processing {}-{} records'.format(start, start+sizeByCore))\n",
    "        x_id, issue = detectLanguage.remote(covidDF['body_text'][start:start+sizeByCore])\n",
    "    else:\n",
    "        print('Processing remaining {}-{} records'.format(start, length))\n",
    "        x_id, issue = detectLanguage.remote(covidDF['body_text'][start:length])\n",
    "    lst.append(x_id)\n",
    "    issues.append(issue)\n",
    "    start+=sizeByCore\n",
    "    \n",
    "# Process the remaining files \n",
    "# x_id = detectLanguage.remote(covidDF['body_text'][start:length])\n",
    "# lst.append(x_id)\n",
    "# print('Processing Remaining {}-{} records'.format(start, length))\n",
    "\n",
    "\n",
    "# Block until the tasks are done and get the results.\n",
    "# x = ray.get(lst)\n",
    "languageLst = []\n",
    "for i in tqdm(lst, desc=\"Unpacking distributed load for languages\"):\n",
    "    languageLst.extend(ray.get(i))\n",
    "\n",
    "issue_lst = []\n",
    "for i in tqdm(issues, desc=\"Unpacking distributed load for issues\"):\n",
    "    issue_lst.append(ray.get(i))\n",
    "    \n",
    "# Shutdown ray     \n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze any issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paper_id': [29],\n",
       "  'body_text': [\",%68)6)( &= 8,) ?140)1)28-2+ )'311)2(%8-327 3* 8,) 311-77-32 '8 3* @ 9&0-' %;8,) %8-32%0 -3 796:)-00%2') 28)+6%8-32 )28)6 -7 ,397)( ;-8,-2 8,) )4%68 1)28 3* 31)0%2( )'96-8= !,) 1-77-32 3* -7 83 )2%&0) )%60= ;%62-2+ %2( 7,%6)( 7-89%8-32%0 %;%6)2)77 3* % %'-0-8%8-2+ '300%&36%8-32 %003;)( 7,%6)( 7-89%8-32%0 %;%6)2)77 %2( )2,%2')( ()'-7-32 1%/-2+ &= 7)2-36 0)%()67,-4 3* *)()6%0 %+)2'-)7'336 (-2%8) \"]},\n",
       " {'paper_id': [78], 'body_text': [' . . . . . . 507 ']}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in issue_lst if len(i['paper_id']) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Length of items in List of Languages:\n",
      "8876\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'en': 8611,\n",
       "         'de': 102,\n",
       "         'fr': 62,\n",
       "         'nl': 7,\n",
       "         'es': 84,\n",
       "         'it': 3,\n",
       "         'pt': 3,\n",
       "         'No text found': 2,\n",
       "         'cy': 1,\n",
       "         'ru': 1})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter \n",
    "\n",
    "print('''\n",
    "Length of items in List of Languages:\n",
    "{}\n",
    "'''.format(len(languageLst)))\n",
    "\n",
    "Counter(languageLst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add language feature to the dataframe\n",
    "\n",
    "Filter the dataframe to select only english documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8611 entries, 0 to 9998\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   paper_id         8611 non-null   object\n",
      " 1   title            8504 non-null   object\n",
      " 2   body_text        8611 non-null   object\n",
      " 3   abstract         8611 non-null   object\n",
      " 4   authors          7852 non-null   object\n",
      " 5   url              8611 non-null   object\n",
      " 6   doi              8611 non-null   object\n",
      " 7   abstract_cnt     8611 non-null   int64 \n",
      " 8   body_cnt         8611 non-null   int64 \n",
      " 9   unique_body_cnt  8611 non-null   int64 \n",
      " 10  language         8611 non-null   object\n",
      "dtypes: int64(3), object(8)\n",
      "memory usage: 807.3+ KB\n"
     ]
    }
   ],
   "source": [
    "covidDF['language'] = languageLst\n",
    "\n",
    "covidDF = covidDF[covidDF['language'] == 'en']\n",
    "covidDF.info()\n",
    "\n",
    "# Delete Unused lists\n",
    "del issue_lst\n",
    "del languageLst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA CLEANING\n",
    "\n",
    "* Remove stop words\n",
    "* Remove punctuation\n",
    "* Use Lemmatization\n",
    "\n",
    "## STOP WORDS\n",
    "\n",
    "\n",
    "Custom medical stop words were recommended by: \n",
    "\n",
    "https://www.kaggle.com/danielwolffram/topic-modeling-finding-related-articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_sci_lg\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import string\n",
    "\n",
    "\n",
    "stop_words = list(STOP_WORDS)\n",
    "medical_stop_words = ['doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al',\n",
    "                      'author', 'figure', 'rights', 'reserved', 'permission', 'used', 'using',\n",
    "                      'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', 'CZI', 'www']\n",
    "\n",
    "stop_words.extend(medical_stop_words) # Converge the 2 lists \n",
    "stop_words = list(set(stop_words)) # Remove duplicates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Clean up function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = en_core_sci_lg.load(disable=['tagger','ner'])\n",
    "#nlp.max_length = 7000000\n",
    "\n",
    "@ray.remote\n",
    "def cleanUp(lst_text):\n",
    "    \n",
    "    nlp = en_core_sci_lg.load(disable=['tagger','ner'])\n",
    "    nlp.max_length = 7000000\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for text in tqdm(lst_text):\n",
    "        tokens = nlp(text)\n",
    "        tokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in tokens]\n",
    "        tokens = [ word for word in tokens if word not in stop_words and word not in string.punctuation ]\n",
    "        tokens = ' '.join([i for i in tokens])\n",
    "        results.append(tokens)\n",
    "        \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cleanUp(text):\n",
    "    \n",
    "#     nlp = en_core_sci_lg.load(disable=['tagger','ner'])\n",
    "#     nlp.max_length = 7000000\n",
    "    \n",
    "\n",
    "#     tokens = nlp(text)\n",
    "#     tokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in tokens]\n",
    "#     tokens = [ word for word in tokens if word not in stop_words and word not in string.punctuation ]\n",
    "#     tokens = ' '.join([i for i in tokens])\n",
    "        \n",
    "#     return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCESS WITH RAY SO IT'S FASTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-16 01:00:28,829\tINFO services.py:1166 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3131115d527248408287a38382d1e3b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Sending parallel processes'), FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0-717 records\n",
      "Processing 717-1434 records\n",
      "Processing 1434-2151 records\n",
      "Processing 2151-2868 records\n",
      "Processing 2868-3585 records\n",
      "Processing 3585-4302 records\n",
      "Processing 4302-5019 records\n",
      "Processing 5019-5736 records\n",
      "Processing 5736-6453 records\n",
      "Processing 6453-7170 records\n",
      "Processing 7170-7887 records\n",
      "Processing remaining 7887-8611 records\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41676be91b2c4902ba2498e89bb8e932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Unpacking distributed load for cleaned text'), FloatProgress(value=0.0, max=12.0), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=8678)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=717.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8673)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=717.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8667)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=717.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8676)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=724.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8672)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=717.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8674)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=717.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8669)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=717.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8675)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=717.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8670)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=717.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8677)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=717.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8671)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=717.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8668)\u001b[0m HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=717.0), HTML(value='')))\n",
      "\u001b[2m\u001b[36m(pid=8677)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8670)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8671)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8673)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8668)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8667)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8669)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8678)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8672)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8675)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8674)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=8676)\u001b[0m \n",
      "\n",
      "CPU times: user 20.8 s, sys: 7.17 s, total: 27.9 s\n",
      "Wall time: 11min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# initialize ray\n",
    "ray.init()\n",
    "\n",
    "# Number of records to be processed\n",
    "length = len(covidDF['body_text'])\n",
    "\n",
    "# Records that will be processed by core\n",
    "# divide by the length \n",
    "sizeByCore = int(length/cores)\n",
    "\n",
    "# Initialize the list that will capture the processed results\n",
    "lst = []\n",
    "\n",
    "\n",
    "# The following for function will loop through all the records \n",
    "# it will be incremented by the number of records by core \n",
    "# Since it is a parallel process, it will use all cores of the machines to process the records distributed \n",
    "# \"evenly\"\n",
    "start = 0\n",
    "for i in tqdm(range(sizeByCore, length, sizeByCore), desc=\"Sending parallel processes\"):\n",
    "    if length-(start+sizeByCore) > sizeByCore:\n",
    "        print('Processing {}-{} records'.format(start, start+sizeByCore))\n",
    "        x_id = cleanUp.remote(covidDF['body_text'][start:start+sizeByCore])\n",
    "    else:\n",
    "        print('Processing remaining {}-{} records'.format(start, length))\n",
    "        x_id = cleanUp.remote(covidDF['body_text'][start:length])\n",
    "    lst.append(x_id)\n",
    "    start+=sizeByCore\n",
    "    \n",
    "# Block until the tasks are done and get the results.\n",
    "# x = ray.get(lst)\n",
    "processedLst = []\n",
    "for i in tqdm(lst, desc=\"Unpacking distributed load for cleaned text\"):\n",
    "    processedLst.extend(ray.get(i))\n",
    "\n",
    "\n",
    "    \n",
    "# Shutdown ray     \n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8611 entries, 0 to 9998\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   paper_id         8611 non-null   object\n",
      " 1   title            8504 non-null   object\n",
      " 2   body_text        8611 non-null   object\n",
      " 3   abstract         8611 non-null   object\n",
      " 4   authors          7852 non-null   object\n",
      " 5   url              8611 non-null   object\n",
      " 6   doi              8611 non-null   object\n",
      " 7   abstract_cnt     8611 non-null   int64 \n",
      " 8   body_cnt         8611 non-null   int64 \n",
      " 9   unique_body_cnt  8611 non-null   int64 \n",
      " 10  language         8611 non-null   object\n",
      " 11  cleaned_text     8611 non-null   object\n",
      "dtypes: int64(3), object(9)\n",
      "memory usage: 874.6+ KB\n"
     ]
    }
   ],
   "source": [
    "covidDF['cleaned_text'] = processedLst\n",
    "covidDF.info()\n",
    "\n",
    "del processedLst\n",
    "del lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spot check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a1111111111 a1111111111 a1111111111 a1111111111 a1111111111 available datum 78 addition significant increase level lactate dehydrogenase α-hydroxybutyrate dehydrogenase detect 28 patient 34 patient available datum 82 25 patient 34 patient available datum 74 respectively patchy lesion lobule detect chest compute tomographic scan 28 patient 82 ground-glass opacity typical feature adult rare pediatric patient 3 rapid radiologic progression late-onset pattern lesion lobule notice lesions lobule exist 24 32 patient lesion 75 patient discharge main symptom disappear day treatment patient discharge median duration hospitalization 10.00 8.00 14.25 day current study limit small sample size lack dynamic detection inflammatory marker datum systemically present clinical epidemiological feature good outcome pediatric patient covid-19 stratified analysis perform mild moderate case finding offer new insight early identification intervention pediatric patient covid-19.why study done?• severe acute respiratory syndrome coronavirus 2 sars-cov-2 infection spread rapidly worldwide.• early identification intervention necessary effective control epidemic adult child information clinical epidemiological characteristic pediatric patient limit researcher find?• collect analyze clinical datum 34 pediatric patient coronavirus disease-2019 covid- 19 4 hospital china january 27 february 23 2020.• describe clinical epidemiological characteristic patient focus feature initial symptom radiological finding outcomes.• contrast observe adult patient high proportion fever vomit diarrhea notice admission pediatric case patchy shadow high density common lobule lesion typical feature ground-glass opacity adult rare pediatric cases.• late-onset pattern lobule lesion reveal basis chest compute tomographic scan clinical presentation severe sign observe compute tomography ct image recovery lesion lobule lag main symptom severe acute respiratory syndrome coronavirus 2 sars-cov-2 infection spread worldwide rapidly emergence wuhan china early december 2019 1 sars-cov-2 epidemic declare public health emergency international concern world health organization january 30 2020 2 date 2,000,000 patient diagnose coronavirus disease-2019 covid- 19 globally cumulative numb laboratory-confirmed case report 660,000 united states 180,000 spain 170,000 italy 80,000 china 3 sars-cov-2 identify diverse clade derive severe acute respiratory syndrome coronavirus sars-cov middle east respiratory syndrome coronavirus mers-cov report cause covid-19 4 clinical characteristic adult patient covid-19 reveal recent study mainly include fever cough dyspnea radiographic finding pneumonia 5 6 7 information pediatric patient limit case series describe clinical epidemiological feature 34 pediatric patient basis epidemiological demographic laboratory radiological datum aim contribute comprehensive understand characteristic covid-19.this retrospective observational study approve institutional review board irb affiliated taihe hospital hubei university medicine ethical approval no. 2020ky01 suspected patient clinical and/or radiological feature pneumonia quarantine prior sars-cov-2 nucleic acid detection accord guideline case suspect infection 8 good instruction pediatric branch hubei medical association pediatric case 9 specifically suspect case sars-cov-2 infection meet 1 follow criterium 10 1 little 1 clinical symptom include fever expectation tachypnea lethargy poor feed cough vomit diarrhea 2 chest radiologic abnormality consistent viral pneumonia diagnosis confirm sars-cov-2 nucleic acid test sample respiratory tract swab admitted child laboratory-confirmed sars-cov-2-positive result 4 hospital west china january 27 february 23 2020 include clinical type disease s1 table assess patient accord recommendation national healthcommission people republic china nhc 10 patients discharge follow criterium meet 10 1 fever recover little 3 day 2 upper respiratory symptom alleviate 3 exudative lesion alleviate significantly accord radiological evidence 4 negative result obtain sars-cov-2 nucleic acid detection 2 consecutive test perform interval 24 hour final follow-up visit complete march 16 2020 assent gain school-aged child write inform consent provide parent guardian prior datum collection prespecified protocol prior current study clinical process datum analysis plan 1 samples nasopharyngeal throat swab sars-cov-2 detection sars-cov-2 rna detect real-time reverse transcription polymerase chain reaction rt-pcr s1 text accordance recommendation nhc 11 result sars-cov-2 nucleic acid detection analyze follow manufacturer instruction cases negative result double check resample retest interval 24 hour confirm negative result obtain 2 consecutive test series laboratory test conduct include hematological serum biochemical acute-phase protein erythrocyte sedimentation rate esr test particular sample nasopharyngeal throat swab test common respiratory pathogen include influenza b virus respiratory syncytial virus adenovirus parainfluenza virus epstein-barr virus mycoplasma pneumonia use rt-pcr assay establish method patients undergo chest compute tomography ct scan radiologic assessment conduct taihe hospital treatment center covid-19 designate local municipal government image store picture archive communication system pacs review 2 experience pediatric radiologist independently 3 radiologist review ct finding confirmation medical record include patient access study team datum collection clinical datum extract include demographic datum medical history epidemiological history underlie disease clinical symptom sign laboratory finding radiological characteristic treatment outcome particular exposure history investigate patient meet follow criterium 10 1 travel history wuhan neighbor area area persistent local transmission 14 day prior disease onset 2 sars-cov-2 infection diagnosis child family caregiver 3 close contact people confirm sars-cov-2 infection patient unexplained pneumonia 4 child associate cluster outbreak addition mix infection define concurrent infection patient 2 pathogen researcher institute drug clinical trials taihe hospital cross-check collect datum ensure quality control communicate attend doctor healthcare provider question study report base strobe checklist s1 strobe checklist).descriptive statistic determine use spss software version 20.0 ibm https://www ibm.com/analytics/spss-statistics-software armonk ny usa imputation miss datum categorical variable present numb frequency rate continuous variable present median interquartile range (iqr).in study 57 suspect pediatric patient screen 34 patient confirm covid-19 enroll 1 include 14 male patient 41 20 female patient 59 \\ufeff1 patient diagnose sars-cov-2 infection january 27 9 day father diagnose covid-19 21 case 62 diagnose 15 february uptrend daily confirm case observe cutoff date recruitment phase 2 median age 33 iqr 10.00 94.25 month range 1 144 month eighteen patient 52 exposure resident wuhan addition 13 38 patient close contact family member covid-19 16 48 patient note history exposure identify source particular mix infection respiratory pathogen report 16 patient 47 include m. pneumonia 26 influenza b virus 18 influenza virus 9 respiratory syncytial virus 6 epstein-barr virus 6 parainfluenza virus 3 adenovirus 3 comorbidities report 6 patient 18 respect initial symptom sign fever 76 cough 62 frequently complaint expectoration 21 tachypnea 9 vomit 12 diarrhea 12 report good patients study present mild 18 moderate 82 form disease moderate case predominant 96 23 patient old 72 month table 1 .on admission hematological test indicate lymphocyte count increase 17 patient 50 median value 3.19 1.73 4.34 normal range concerning finding blood biochemistry prealbumin median 138.65 mg/l decrease significantly 25 patient 32 patient available datum 78 substantial increase detect serum amyloid saa 17 patient 20 patient available datum 85 high-sensitivity c-reactive protein hs-crp 17 patient 29 patient available datum 59 addition noticeable increase observe lactate dehydrogenase ldh 28 patient 34 patient 82 α-hydroxybutyrate dehydrogenase α-hbdh 25 patient 34 patient 74 result creatine kinase ck creatine kinase-mb ck-mb normal patient table 2 significant finding observe routine blood coagulation test urine stool test result electrocardiogram ecg exam normal patient hospitalization hospitalization laboratory test review 7 day admission level saa hs-crp prealbumin recover 7 day posttreatment patient abnormality baseline median duration recovery 7.00 7.00 10.00 day ldh 8.00 7.00 10.00 day α-hbdh proportion patient recover ldh α-hbdh 86 24 patient 84 21 patient respectively lesions lobule characterize patchy shadow high density indicate chest ct scan 28 patient 82 admission ground-glass opacity patchy shadow observe 1 case 3 study table 3 unilateral lesion 41 bilateral lesion 41 detect patient radiological finding notable lesion progression detect 18 28 patient lesion admission 64 patient hospitalization patient 1 3 late-onset pattern chest ct image observe 4 case 34 patient 12 patient normal initial ct image admission lesion lobule emerge 4 5 day patient 2 3 2 case 34 patient 6 emergence lesion hospitalization antiviral treatment employ accord recommendation nhc 10 mild moderate case patient receive interferon-α nebulization twice day ribavirin 15 44 patient twice day addition 20 59 patient receive traditional chinese medicine antibiotics 11 patient initial diagnosis bacterial pneumonia admission detection sars-cov-2 infection withdraw confirmation covid-19 patient receive antibiotic therapy concern viral-bacterial mix infection hospitalization azithromycin 9 patient m. pneumonia infection corticosteroid 15 oxygen inhalation supportive therapy 9 employ table 3 patient discharge main symptom disappear sars-cov-2 test negative lesion lobule recover 8 patient lesion exist 24 patient 32 patient lesion 75 discharge 4 duration fever 3.00 2.00 4.00 day similar cough 4.00 day 2.00 7.00 duration hospitalization 10.00 8.00 14.25 day patient short duration hospitalization indicate mild case 8.00 day 7.00 9.50 moderate case 10.50 day 8.00 15.00 table 4 .along rapid spread sars-cov-2 infection pediatric case covid-19 gradually increase morbidity covid-19 child report 0.9 china 1 1.2 italy 12 5 usa 13 clinical epidemiological characteristic pediatric patient determine clearly report clinical epidemiological feature 34 pediatric patient covid-19 age 1 144 month patients experience mild moderate disease form current study patient suffer fever cough recover 3.00 4.00 day treatment progression pattern lesion lobule reveal chest ct scan lesion exist majority patient discharge unlike report typical feature ground-glass opacity observe adult rare pediatric patient base datum substantial increase detect ssa hs-crp ldh α-hdbd recover promptly treatment current study find patient present mild moderate covid-19 disease consistent result previous study 14 15 report 94 case identify asymptomatic 4 mild 51 moderate 39 2,143 confirm suspect pediatric patient china 14 underlie mechanism mild disease presentation child compare adult topic research hypothesis raise base current understand covid-19 possible explanation relate reduce inflammatory response little well-developed immune system child adult 16 substantial increase hs-crp detect 59 case study similar observe adult case 61 1 find suggest immunological response similar adult occur pediatric population neighbor area wuhan support immature immune system theory serum inflammatory marker detection perform study limitation retrospective study design detection helpful address controversial issue future theory originate observation young child experience mild disease course children young age tend viral infection possible repeat viral exposure strengthen immune system respond sars-cov-2 17 correspondingly mix infection detect 16 47 patient pathogen include m. pneumonia influenza b virus respiratory syncytial virus epstein-barr virus parainfluenza virus adenovirus pathogen test negative 10 pediatric case guangzhou 18 stratified analysis accord age range perform determine correlation age mix infection good impact mix infection clinical type disease result mix infection 62 13 patient common child age 12 72 month 12 moderate case 92 13 patient identify subgroup suggest mix infection increase protection ameliorate disease course covid-19 base datum addition child moderate disease age 72 month account 79 moderate case suggest preschool child prone develope sars-cov-2 infection according current datum chest ct image patchy shadow detect 82 patient admission accordance previous report adult 86 1 child 65 15 lesions lobule characterize patchy shadow high density case 97 ground-glass opacity rare 3 current study common pediatric case wuhan 33 171 case 15 guangzhou 50 10 case 18 good adult 56 19 notably proportion patient history exposure 52 current study proportion 90 171 case wuhan 15 100 10 case guangzhou 18 exposure status attribute partially discrepancy proportion pediatric case current study previous study study need reveal correlativity viral load sars-cov-2 exposure status identify underlie reason discrepancy time course lung change reveal adult patient 20 course progression remain elusive pediatric case notably severe progression lesion lobule notice 7 day admission current study appear 4 5 day admission clinical presentation severe sign ct image rapid radiologic progression report peak approximately 2 week onset 21 adult case addition late-onset pattern lesion detect case lesion indicate ct scan approximately 7 day symptom onset similar observe report adult 6 12 day 22 nonetheless finding suggest close monitor pediatric patient perform severe progression lesion lobule late-onset pattern case level saa find increase high percentage 17 patient 20 patient available datum 85 patient undergo test sensitive marker correlate extent pneumonia sars patient 23 level hs-crp saa recover dramatically 7 day treatment correlation saa sars-cov-2 infection remain investigate pediatric patient consistent previous report adult 1 6 child 24 level ldh α-hbdh increase symptom sign myocardial impairment respect initial symptom fever identify 26 child 76 study present 44 adult patient admission 1 addition vomit 12 diarrhea 12 present admission common child adult patient 5 vomit 4 diarrhea 1 comorbidities find 6 patient 18 current study similar observe adult patient mild symptom 21 1 .the therapeutic strategy base antiviral therapy alignment recommendation nhc 10 patient recover main symptom discharge negative sars-cov-2 detection result achieve 10.00 8.00 14.25 day lesions lobule exist 75 patient great improvement ct scan treatment association radiologic finding mortality reveal adult patient 19 suggest utilize ct scan prognosis prediction mild moderate case definitive correlation find radiologic image course disease study study add new information exist report epidemiological characteristic considerable percentage pediatric patient 48 notice unidentified source infection 72 nonresidents wuhan contact resident wuhan 1 unanticipated finding suggest reference value exposure history epidemic area early identification sars-cov-2 infection consider carefully pediatric patient rapid development epidemic correlation exposure history disease severity investigate future study large population accordance present study 14 25 family cluster transmission find common pediatric patient report infection dynamic pediatric patient caregiver transmission adult child identify confirm evidence children potential spreader explosive stage outbreak attribute high prevalence asymptomatic infection mild disease pediatric population 25 close monitor track system involve hospital community utilize track transmission pediatric patient caregiver evidence regard transmission route pediatric patient caregiver closecontact family member patient population current study representative pediatric case diagnose treat west china interpretation finding limit small sample size retrospective study design underlie reason low risk severe form covid-19 child remain elusive lack dynamic detection viral load sars-cov-2 inflammatory marker information issue help obtain broad view covid-19.this case series describe clinical epidemiological characteristic pediatric patient covid-19 datum present clinical feature pediatric patient facilitate early identification intervention suspect patient notwithstanding relatively limit numb sample finding offer valuable insight early diagnosis epidemic control covid-19 child'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covidDF.iloc[0]['cleaned_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     8611.000000\n",
       "mean      1000.797584\n",
       "std        808.436791\n",
       "min         13.000000\n",
       "25%        586.000000\n",
       "50%        884.000000\n",
       "75%       1225.000000\n",
       "max      14850.000000\n",
       "Name: unique_body_cnt, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "covidDF['unique_body_cnt'].hist()\n",
    "covidDF['unique_body_cnt'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LATENT DIRILECTH ALLOCATION\n",
    "\n",
    "## USING SCIKIT \n",
    "\n",
    "In order to do topic modeling, we need to see what is the optimal number of topics \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use k-means to get a number of topics\n",
    "\n",
    "### Vectorization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.5 s, sys: 385 ms, total: 13.9 s\n",
      "Wall time: 14.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8611, 4096)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def vectorization(text, _max_features):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=_max_features)\n",
    "    result = vectorizer.fit_transform(text)\n",
    "    return result\n",
    "\n",
    "\n",
    "text = covidDF['cleaned_text'].values\n",
    "X = vectorization(text, 2 ** 12)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA principle component analysis \n",
    "\n",
    "Reduce dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 47s, sys: 13 s, total: 5min\n",
      "Wall time: 54.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8611, 2285)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X_red= pca.fit_transform(X.toarray())\n",
    "X_red.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means to choose number of topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5325f73726bf418dbe45282b1eb45c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=48.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1070\u001b[0m                 \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m                 \u001b[0mx_squared_norms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_squared_norms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m                 n_threads=self._n_threads)\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0;31m# determine if these results are the best so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbest_inertia\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minertia\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_inertia\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36m_kmeans_single_elkan\u001b[0;34m(X, sample_weight, n_clusters, max_iter, init, verbose, x_squared_norms, random_state, tol, n_threads)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     init_bounds(X, centers, center_half_distances,\n\u001b[0;32m--> 416\u001b[0;31m                 labels, upper_bounds, lower_bounds)\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0mstrict_convergence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "start = 2\n",
    "end = 50\n",
    "\n",
    "#Iterate through different k values \n",
    "distortions = []\n",
    "for k in tqdm(range(start, end)):\n",
    "    #k_means = KMeans(n_clusters=k, random_state=42, n_jobs=-1).fit(X_red)\n",
    "    k_means = KMeans(n_clusters=k, random_state=42).fit(X_red)\n",
    "    k_means.fit(X_red)\n",
    "    distortions.append(sum(np.min(cdist(X_red, k_means.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "    #print('found distortions for {} clusters'.format(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use dask to speed up processing \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:51099</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>10</li>\n",
       "  <li><b>Cores: </b>10</li>\n",
       "  <li><b>Memory: </b>17.18 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:51099' processes=10 threads=10, memory=17.18 GB>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://examples.dask.org/machine-learning/incremental.html\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client(n_workers=10, threads_per_worker=1)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed9362916a104d63bd1ef3357ef21b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=48.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-5bb5b35ad846>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#k_means = KMeans(n_clusters=k, random_state=42, n_jobs=-1).fit(X_red)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mk_means\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_red\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mk_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_red\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdistortions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_red\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'euclidean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/dask_ml/cluster/k_means.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0minit_max_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_max_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         )\n\u001b[1;32m    203\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/dask_ml/cluster/k_means.py\u001b[0m in \u001b[0;36mk_means\u001b[0;34m(X, n_clusters, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter, oversampling_factor, init_max_iter)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0moversampling_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moversampling_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0minit_max_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_max_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m     )\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_n_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/dask_ml/cluster/k_means.py\u001b[0m in \u001b[0;36m_kmeans_single_lloyd\u001b[0;34m(X, n_clusters, max_iter, init, verbose, x_squared_norms, random_state, tol, precompute_distances, oversampling_factor, init_max_iter)\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mnew_centers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_centers\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mnew_centers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_centers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;31m# Convergence check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0mpostcomputes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_postcompute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, dsk, keys, restrictions, loose_restrictions, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\u001b[0m\n\u001b[1;32m   2723\u001b[0m                     \u001b[0mshould_rejoin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2724\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2725\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2726\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2727\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(self, futures, errors, direct, asynchronous)\u001b[0m\n\u001b[1;32m   1990\u001b[0m                 \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1991\u001b[0m                 \u001b[0mlocal_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1992\u001b[0;31m                 \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1993\u001b[0m             )\n\u001b[1;32m   1994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m             return sync(\n\u001b[0;32m--> 833\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m             )\n\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import dask\n",
    "from dask_ml.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "start = 2\n",
    "end = 50\n",
    "\n",
    "#Iterate through different k values \n",
    "distortions = []\n",
    "for k in tqdm(range(start, end)):\n",
    "    #k_means = KMeans(n_clusters=k, random_state=42, n_jobs=-1).fit(X_red)\n",
    "    k_means = KMeans(n_clusters=k, random_state=42).fit(X_red)\n",
    "    k_means.fit(X_red)\n",
    "    distortions.append(sum(np.min(cdist(X_red, k_means.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "    #print('found distortions for {} clusters'.format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "IOLoop is closed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-5c7a182e8533>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1455\u001b[0m         \u001b[0mClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclose\u001b[0m \u001b[0monly\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m         \"\"\"\n\u001b[0;32m-> 1457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_executor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m             return sync(\n\u001b[0;32m--> 833\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m             )\n\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masyncio_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# tornado 6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"IOLoop is closed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: IOLoop is closed"
     ]
    }
   ],
   "source": [
    "client.shutdown()\n",
    "client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "X_axis = [start, end]\n",
    "Y_axis = [max(distortions), min(distortions)]\n",
    "\n",
    "plt1 = go.Scatter(x=[*K], y=distortions)\n",
    "plt2 = go.Scatter(x=X_axis, y=Y_axis)\n",
    "data = [plt1, plt2]\n",
    "\n",
    "\n",
    "fig = go.Figure(data=data)\n",
    "\n",
    "# Edit the layout\n",
    "fig.update_layout(title='Elbow method for optimal number of topics',\n",
    "                   xaxis_title='Number of Topics',\n",
    "                   yaxis_title='Distortion')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LATENT DIRILECTH ALLOCATION IMPLEMENTATION\n",
    "\n",
    "Now that know the number of topics to choose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_df = 0.95, min_df = 2, stop_words = 'english')\n",
    "cv = CountVectorizer(max_df = 0.95, stop_words = 'english')\n",
    "\n",
    "termFrequency = cv.fit_transform(covidDF['cleaned_text'])\n",
    "featureNames = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components = 9, random_state = 42)\n",
    "lda.fit(termFrequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, topic in tqdm(enumerate(lda.components_)):\n",
    "    print(f'Top 15 words for Topic #{index}')\n",
    "    print([featureNames[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/latent-dirichlet-allocation-for-topic-modelling-explained-algorithm-and-python-scikit-learn-c65a82e7304d\n",
    "\n",
    "#covidDF.iloc[1]['cleaned_text']\n",
    "nlp = en_core_sci_lg.load(disable=['tagger','ner'])\n",
    "nlp.max_length = 7000000\n",
    "\n",
    "\n",
    "sample_txt = '''COVID-19: Psychiatrists warn of coronavirus lockdown's toll on mental health'''\n",
    "\n",
    "tokens = nlp(covidDF.iloc[0]['cleaned_text'])\n",
    "tokens = nlp(sample_txt)\n",
    "tokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in tokens]\n",
    "tokens = [ word for word in tokens if word not in stop_words and word not in string.punctuation ]\n",
    "#tokens = ' '.join([i for i in tokens])\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.transform(cv.transform(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "\n",
    "freq_lst = []\n",
    "for i in lda.transform(cv.transform(tokens)):\n",
    "    print(i)\n",
    "    freq_lst.append(i.argmax())\n",
    "    \n",
    "cnt = Counter(freq_lst)\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dict = {'topic_1': [],\n",
    "             'topic_2':[],\n",
    "             'topic_3':[], \n",
    "             'topic_4':[],\n",
    "             'topic_5': [],\n",
    "             'topic_6': [],\n",
    "             'topic_7': []}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(Counter(freq_lst).values())\n",
    "\n",
    "print(Counter(freq_lst).keys())\n",
    "print([i/sum(Counter(freq_lst).values()) for i in Counter(freq_lst).values()])\n",
    "\n",
    "total = sum(cnt.values())\n",
    "\n",
    "for i in range(1, len(temp_dict)+1):\n",
    "    print(i)\n",
    "    try:\n",
    "        temp_dict['topic_'+str(i)].append(cnt[i]/total)\n",
    "    except:\n",
    "        temp_dict['topic_'+str(i)].append(None)\n",
    "        \n",
    "temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/latent-dirichlet-allocation-for-topic-modelling-explained-algorithm-and-python-scikit-learn-c65a82e7304d\n",
    "\n",
    "#covidDF.iloc[1]['cleaned_text']\n",
    "nlp = en_core_sci_lg.load(disable=['tagger','ner'])\n",
    "nlp.max_length = 7000000\n",
    "\n",
    "\n",
    "sample_txt = '''COVID-19: Psychiatrists warn of coronavirus lockdown's toll on mental health'''\n",
    "\n",
    "tokens = nlp(covidDF.iloc[0]['cleaned_text'])\n",
    "tokens = nlp(sample_txt)\n",
    "tokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in tokens]\n",
    "tokens = [ word for word in tokens if word not in stop_words and word not in string.punctuation ]\n",
    "#tokens = ' '.join([i for i in tokens])\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USING GENSIM \n",
    "\n",
    "https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0\n",
    "\n",
    "https://github.com/kapadias/mediumposts/blob/master/nlp/published_notebooks/Evaluate%20Topic%20Models.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(covidDF['cleaned_text'].str.split(' '))\n",
    "\n",
    "# Create Corpus\n",
    "texts = covidDF['cleaned_text'].str.split(' ')\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1][0][:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=10, \n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "we can move on to using TF-IDF which is pretty straightforward to do in sklearn\n",
    "\n",
    "We can maybe use \n",
    "LATENT DIRECHT ALLOCATION\n",
    "https://www.youtube.com/watch?v=3mHy4OSyRf0&t=515s\n",
    "\n",
    "https://medium.com/analytics-vidhya/topic-modelling-using-latent-dirichlet-allocation-in-scikit-learn-7daf770406c4\n",
    "\n",
    "\n",
    "Both K-means and Latent Dirichlet Allocation (LDA) are unsupervised learning algorithms, where the user needs to decide a priori the parameter K, respectively the number of clusters and the number of topics.\n",
    "\n",
    "If both are applied to assign K topics to a set of N documents, the most evident difference is that K-means is going to partition the N documents in K disjoint clusters (i.e. topics in this case). On the other hand, LDA assigns a document to a mixture of topics. Therefore each document is characterized by one or more topics (e.g. Document D belongs for 60% to Topic A, 30% to topic B and 10% to topic E). Hence, LDA can give more realistic results than k-means for topic assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our objective is to produce target labels, this is a perfect application for an unsupervised learning algorithm.  We want to segment the documents into different groups so that we can create an api to route to the appropriate cluster. \n",
    "\n",
    "We randomly initialize the K starting centroids. Each data point is assigned to its nearest centroid.\n",
    "The centroids are recomputed as the mean of the data points assigned to the respective cluster.\n",
    "Repeat steps 1 and 2 until we trigger our stopping criteria.\n",
    "\n",
    "* Term Frequency-Inverse Document Frequency or TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use k-means institution to discover categories that we would not be able to see by ourselves \n",
    "https://towardsdatascience.com/k-means-clustering-8e1e64c1561c\n",
    "    \n",
    "    https://www.kaggle.com/dfoly1/k-means-clustering-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess\n",
    "\n",
    "TRAIN\n",
    "\n",
    "SCORE\n",
    "\n",
    "EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'what on earth'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask.distributed import Client, progress\n",
    "# import dask\n",
    "# import dask.dataframe as dd\n",
    "\n",
    "# #client = Client(n_workers=2, threads_per_worker=2, memory_limit='1GB')\n",
    "# client = Client(n_workers=2, threads_per_worker=2, memory_limit='0.5GB')\n",
    "# client = Client()\n",
    "# client\n",
    "\n",
    "# #df = dask.datasets.timeseries()\n",
    "# #df.head(3)\n",
    "\n",
    "# distcovidDF = dd.from_pandas(covidDF, npartitions=None, chunksize=100)\n",
    "\n",
    "# #client.close()\n",
    "\n",
    "# distcovidDF['cleaned_text'] = distcovidDF['body_text'].apply(cleanUp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
